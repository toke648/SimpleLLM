from tokenizers import Tokenizer
import torch
from torch.utils.data import DataLoader, TensorDataset
import torch.nn as nn
import torch.optim as optim
import os

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# ----------- åŠ è½½ tokenizer -----------
tokenizer = Tokenizer.from_file("bpe_tokenizer.json")
vocab_size = tokenizer.get_vocab_size()
print("è¯è¡¨å¤§å°:", vocab_size)

# ----------- æ•°æ®é›†å¤„ç†ï¼ˆé—® â†’ ç­”ï¼‰-----------
def create_qa_data(file_path, tokenizer, seq_length=50):
    inputs, targets = [], []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            if '\t' in line:
                q, a = line.strip().split('\t')
                q_ids = tokenizer.encode("<bos> " + q + " <eos>").ids
                a_ids = tokenizer.encode("<bos> " + a + " <eos>").ids

                # padding åˆ°å›ºå®šé•¿åº¦
                q_ids = q_ids[:seq_length] + [0]*(seq_length - len(q_ids)) if len(q_ids) < seq_length else q_ids[:seq_length]
                a_ids = a_ids[:seq_length] + [0]*(seq_length - len(a_ids)) if len(a_ids) < seq_length else a_ids[:seq_length]

                inputs.append(q_ids)
                targets.append(a_ids)

    return torch.tensor(inputs), torch.tensor(targets)

inputs, targets = create_qa_data("./processed_data.txt", tokenizer, seq_length=50)
print("è®­ç»ƒæ•°æ®å¤§å°:", inputs.shape, targets.shape)

# dataset = TensorDataset(inputs, targets)
# dataloader = DataLoader(dataset, batch_size=16, shuffle=True)

# åªå– 5000 æ¡è®­ç»ƒçœ‹çœ‹
subset_inputs = inputs[:5000]
subset_targets = targets[:5000]
dataset = TensorDataset(subset_inputs, subset_targets)
dataloader = DataLoader(dataset, batch_size=16, shuffle=True)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("ä½¿ç”¨è®¾å¤‡:", device)

# ----------- æ¨¡å‹ï¼ˆEncoder-Decoderï¼‰-----------
class TransformerChatbot(nn.Module):
    def __init__(self, vocab_size, d_model, num_heads, num_layers, dim_feedforward, max_len):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model) # è¯å‘é‡åµŒå…¥
        self.positional_encoding = nn.Parameter(torch.randn(1, max_len, d_model)) # ä½ç½®ç¼–ç 

        encoder_layer = nn.TransformerEncoderLayer(d_model, num_heads, dim_feedforward) # TransformerEncoderLayerå±‚: ç¼–ç å™¨å±‚
        decoder_layer = nn.TransformerDecoderLayer(d_model, num_heads, dim_feedforward) # TransformerDecoderLayerå±‚: è§£ç å™¨å±‚

        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers) # TransformerEncoder: ç¼–ç å™¨
        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers) # TransformerDecoder: è§£ç å™¨
        self.fc_out = nn.Linear(d_model, vocab_size)

    def forward(self, src, tgt):
        # src: è¾“å…¥åºåˆ—, tgt: ç›®æ ‡åºåˆ—
        src_emb = self.embedding(src) + self.positional_encoding[:, :src.size(1), :]
        tgt_emb = self.embedding(tgt) + self.positional_encoding[:, :tgt.size(1), :]

        # è½¬ç½®ä¸º (seq_len, batch_size, d_model) ä»¥é€‚åº” nn.TransformerEncoder å’Œ nn.TransformerDecoder çš„è¾“å…¥æ ¼å¼
        src_emb = src_emb.transpose(0,1)
        tgt_emb = tgt_emb.transpose(0,1)
        
        # ç¼–ç å™¨å’Œè§£ç å™¨çš„å‰å‘ä¼ æ’­
        memory = self.encoder(src_emb)
        output = self.decoder(tgt_emb, memory)
        output = output.transpose(0,1)
        return self.fc_out(output)

d_model = 64
num_heads = 4
num_layers = 2
dim_feedforward = 128
max_len = inputs.shape[1]

model = TransformerChatbot(vocab_size, d_model, num_heads, num_layers, dim_feedforward, max_len).to(device)

optimizer = optim.Adam(model.parameters(), lr=0.001)
loss_fn = nn.CrossEntropyLoss(ignore_index=0)  # å¿½ç•¥ pad

# ----------- è®­ç»ƒå‡½æ•° -----------
def train(model, dataloader, epochs=10):
    for epoch in range(epochs):
        model.train()
        epoch_loss = 0
        for src, tgt in dataloader:
            src, tgt = src.to(device), tgt.to(device)
            optimizer.zero_grad()

            output = model(src, tgt[:, :-1])
            loss = loss_fn(output.reshape(-1, vocab_size), tgt[:,1:].reshape(-1))
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        print(f"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(dataloader):.4f}")

    torch.save(model.state_dict(), "chatbot_model.pth")

train(model, dataloader, epochs=20)

# ----------- ç”Ÿæˆå‡½æ•° -----------
def generate_reply(model, tokenizer, input_text, max_length=50):
    model.load_state_dict(torch.load("chatbot_model.pth", map_location=device))
    model.eval()

    input_ids = tokenizer.encode("<bos> " + input_text + " <eos>").ids
    input_ids = input_ids[:max_length] + [0]*(max_length - len(input_ids)) if len(input_ids) < max_length else input_ids[:max_length]
    input_tensor = torch.tensor([input_ids]).to(device)

    generated_ids = [tokenizer.token_to_id("<bos>")]
    for _ in range(max_length):
        tgt_tensor = torch.tensor([generated_ids]).to(device)

        with torch.no_grad():
            output = model(input_tensor, tgt_tensor)
            next_token = output[0, -1].argmax().item()

        if next_token == tokenizer.token_to_id("<eos>") or next_token == 0:
            break
        generated_ids.append(next_token)

    return tokenizer.decode(generated_ids)

# ----------- æµ‹è¯•èŠå¤© -----------
while True:
    user_input = input("ğŸ‘¤ ä½ ï¼š")
    reply = generate_reply(model, tokenizer, user_input)
    print("ğŸ¤– AIï¼š", reply)
